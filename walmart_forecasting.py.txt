# -*- coding: utf-8 -*-
"""Walmart15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dUWEp-JJT4v5bV1622xjCyqXK4uY2whw
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

train_df = pd.read_csv("train.csv")
features_df = pd.read_csv("features.csv")
stores_df = pd.read_csv("stores.csv")

# Merge the datasets
#drop duplicate IsHoliday column from features to avoid clash
features_df = features_df.drop(columns=['IsHoliday'], errors='ignore')
# Merge train+ features

merged_df = pd.merge(train_df, features_df, on=["Store", "Date"], how="left")
#merge with rows
merged_df = pd.merge(merged_df,stores_df, on="Store", how="left")

# Step 4: Add useful time-based features
merged_df["Date"] = pd.to_datetime(merged_df["Date"])
merged_df["Year"] = merged_df["Date"].dt.year
merged_df["Month"] = merged_df["Date"].dt.month
merged_df["Week"] = merged_df["Date"].dt.isocalendar().week

merged_df.head()

merged_df.sample(10)

# Step 5 (Optional): Save to Excel or CSV
merged_df.to_csv("Walmart_Merged.csv", index=False)
from google.colab import files
files.download("Walmart_Merged.csv")

merged_df.shape # Rows and columns
merged_df.columns# list of features
merged_df.info() # Data types and non null counts
merged_df.describe #summary statistics for numerical features

merged_df.isnull().sum().sort_values(ascending=False)

merged_df.duplicated().sum()

merged_df['Date']= pd.to_datetime(merged_df['Date'])

merged_df['Type']= merged_df['Type'].map({'A':1,'B':2,'C':3})

merged_df['Year'] = merged_df['Date'].dt.year
merged_df['Month'] = merged_df['Date'].dt.month
merged_df['Week'] = merged_df['Date'].dt.isocalendar().week

merged_df = merged_df.sort_values(by=["Store","Dept","Date"])

merged_df.sample(10)

# weekly sales distribution
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 5))
sns.histplot(merged_df['Weekly_Sales'], bins=50, kde=True)
plt.title("Weekly Sales Distribution")
plt.xlabel("Weekly Sales")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

#total sales over time
merged_df['Date'] = pd.to_datetime(merged_df['Date'])
sales_over_time = merged_df.groupby('Date')['Weekly_Sales'].sum()

plt.figure(figsize=(14, 5))
sales_over_time.plot()
plt.title("Total Weekly Sales Over Time")
plt.xlabel("Date")
plt.ylabel("Total Sales")
plt.tight_layout()
plt.show()

#weekly sales by store type
plt.figure(figsize=(8, 5))
sns.boxplot(data=merged_df, x='Type', y='Weekly_Sales')
plt.title("Weekly Sales by Store Type")
plt.tight_layout()
plt.show()

#feature correlation heatmap
plt.figure(figsize=(12, 8))
corr = merged_df.select_dtypes(include='number').corr()
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.tight_layout()
plt.show()

#XGboost model for Walmart sales forecasting
#import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Step 2: Load and merge data
train = pd.read_csv("train.csv")
features = pd.read_csv("features.csv").drop(columns=["IsHoliday"], errors="ignore")
stores = pd.read_csv("stores.csv")

df = train.merge(features, on=["Store", "Date"], how="left")
df = df.merge(stores, on="Store", how="left")

# Step 3: Feature engineering
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Week'] = df['Date'].dt.isocalendar().week.astype(int)

df['Type'] = df['Type'].map({'A': 1, 'B': 2, 'C': 3})

# Sort for lag creation
df = df.sort_values(by=['Store', 'Dept', 'Date'])

# Lag features
df['Sales_last_week'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)
df['Sales_last_year'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(52)

# Drop rows with NA from lag
df = df.dropna()

# Step 4: Define features and target
features = ['Store', 'Dept', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI',
            'Unemployment', 'Type', 'Size', 'Year', 'Month', 'Week',
            'Sales_last_week', 'Sales_last_year']

X = df[features]
y = df['Weekly_Sales']

# Step 5: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Step 6: Train XGBoost model
xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)

# Step 7: Predict and evaluate
y_pred = xgb_model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print("XGBoost RMSE:", rmse)
print("XGBoost MAE:", mae)

import pandas as pd

# Create a comparison DataFrame
xgb_results = pd.DataFrame({
    'Actual Sales': y_test.values,
    'Predicted Sales': y_pred
})

# View top rows
xgb_results.head(10)

import matplotlib.pyplot as plt

plt.figure(figsize=(12,6))
plt.plot(y_test.values[:50], label='Actual Sales', marker='o')
plt.plot(y_pred[:50], label='Predicted Sales', marker='x')
plt.title("XGBoost: Actual vs Predicted Weekly Sales")
plt.xlabel("Sample Index")
plt.ylabel("Weekly Sales")
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Plot Actual vs Predicted Weekly Sales
plt.figure(figsize=(12, 6))
plt.plot(y_test.values, label='Actual Sales', color='navy', marker='o')
plt.plot(y_pred, label='Predicted Sales', color='orange', linestyle='--', marker='x')

# Highlight key trends
plt.axvline(x=30, color='gray', linestyle='--', alpha=0.7)
plt.text(30, max(y_test), 'Sharp Drop Point', rotation=90, verticalalignment='center', fontsize=9, color='gray')

# Add interpretation notes
plt.title("ðŸ“ˆ XGBoost: Actual vs Predicted Weekly Sales")
plt.xlabel("Week Index")
plt.ylabel("Weekly Sales ($)")
plt.legend()
plt.grid(True)

# Annotate the initial trend
plt.annotate('Close Fit Period',
             xy=(10, y_test.values[10]),
             xytext=(5, max(y_test)),
             arrowprops=dict(facecolor='green', shrink=0.05),
             fontsize=10, color='green')

# Annotate the prediction accuracy at low sales
plt.annotate('Well predicted low-sales period',
             xy=(40, y_pred[40]),
             xytext=(35, min(y_test)),
             arrowprops=dict(facecolor='purple', shrink=0.05),
             fontsize=10, color='purple')

plt.tight_layout()
plt.show()

# Step 1: Install LightGBM if not already installed
!pip install lightgbm

# Step 2: Import libraries
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Step 3: Prepare the dataset (reusing from before)

# Define the features
features = ['Store', 'Dept', 'IsHoliday', 'Temperature', 'Fuel_Price', 'CPI',
            'Unemployment', 'Type', 'Size', 'Year', 'Month', 'Week',
            'Sales_last_week', 'Sales_last_year']

X = df[features]
y = df['Weekly_Sales']

# Step 4: Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Step 5: Train LightGBM model
lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
lgb_model.fit(X_train, y_train)

# Step 6: Predict and evaluate
y_pred_lgb = lgb_model.predict(X_test)

rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)

print("LightGBM RMSE:", rmse_lgb)
print("LightGBM MAE:", mae_lgb)

# Step 1: Import Random Forest
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Step 2: Train the Random Forest Model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)

# Step 3: Predict and Evaluate
y_pred_rf = rf_model.predict(X_test)

rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
mae_rf = mean_absolute_error(y_test, y_pred_rf)

print("Random Forest RMSE:", rmse_rf)
print("Random Forest MAE:", mae_rf)

# step 1: install catboost
!pip install Catboost

# Step 2: Import libraries
from catboost import CatBoostRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Step 3: Prepare data
# We assume you're using the same `X_train`, `X_test`, `y_train`, `y_test`
# from the cleaned and preprocessed dataset (lag features, date conversion, etc.)

# Step 4: Train CatBoost Regressor
cat_model = CatBoostRegressor(verbose=0, iterations=100, learning_rate=0.1, random_seed=42)
cat_model.fit(X_train, y_train)

# Step 5: Predict and Evaluate
y_pred_cat = cat_model.predict(X_test)

rmse_cat = np.sqrt(mean_squared_error(y_test, y_pred_cat))
mae_cat = mean_absolute_error(y_test, y_pred_cat)

print("CatBoost RMSE:", rmse_cat)
print("CatBoost MAE:", mae_cat)

from prophet import Prophet
import pandas as pd
import matplotlib.pyplot as plt

# Use the full merged df with date and sales
store_dept_df = df[(df['Store'] == 1) & (df['Dept'] == 1)][['Date', 'Weekly_Sales']]
store_dept_df = store_dept_df.rename(columns={"Date": "ds", "Weekly_Sales": "y"})

model = Prophet()
model.fit(store_dept_df)

future = model.make_future_dataframe(periods=12, freq='W') # 12 weeks ahead
forecast = model.predict(future)

model.plot(forecast)
plt.title("Weekly Sales Forecast (Store 1, Dept 1)")
plt.xlabel("Date")
plt.ylabel("Weekly Sales")
plt.show()

model.plot_components(forecast)
plt.show()

store_dept_df = df[(df['Store'] == 2) & (df['Dept'] == 1)][['Date', 'Weekly_Sales']]
store_dept_df = store_dept_df.rename(columns={"Date": "ds", "Weekly_Sales": "y"})

model = Prophet()
model.fit(store_dept_df)

future = model.make_future_dataframe(periods=12, freq='W') # forecast next 12 weeks
forecast = model.predict(future)

model.plot(forecast)
plt.title("Weekly Sales Forecast (Store 2, Dept 1)")
plt.xlabel("Date")
plt.ylabel("Weekly Sales")
plt.show()

model.plot_components(forecast)
plt.show()

from prophet import Prophet
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Filter Dept 1 or any consistent department across stores
dept_df = merged_df[merged_df['Dept'] == 1]

# Step 2: Prepare trend data for each store
plt.figure(figsize=(16, 8))
for store_id in dept_df['Store'].unique():
    store_data = dept_df[dept_df['Store'] == store_id][['Date', 'Weekly_Sales']].copy()
    store_data = store_data.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})
    store_data = store_data.groupby('ds').sum().reset_index()  # sum all departments if needed

    # Prophet model
    model = Prophet(daily_seasonality=False, weekly_seasonality=True, yearly_seasonality=True)
    model.fit(store_data)

    # Make future dataframe
    future = model.make_future_dataframe(periods=0)  # Just to get the fitted trend

    forecast = model.predict(future)

    # Plot trend
    plt.plot(forecast['ds'], forecast['trend'], label=f'Store {store_id}')

# Final touches
plt.title('Prophet Trend Component for All Stores (Dept 1)')
plt.xlabel('Date')
plt.ylabel('Trend Sales')
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np

# Load your merged dataset again
# Example:
# merged_df = pd.read_csv('/content/Walmart_Merged.csv')

# Filter for Store 1 and Department 1
store1_dept1 = merged_df[(merged_df['Store'] == 1) & (merged_df['Dept'] == 1)].copy()

# Group by week to get weekly sales
weekly_sales = store1_dept1.groupby('Date')['Weekly_Sales'].sum().reset_index()
weekly_sales = weekly_sales.sort_values(by='Date')

# Calculate average and standard deviation of weekly sales
avg_demand = weekly_sales['Weekly_Sales'].mean()
std_demand = weekly_sales['Weekly_Sales'].std()

# Define parameters
lead_time_weeks = 2           # You can change this based on business logic
z_value = 1.65                # For 95% service level

# Calculate Safety Stock and Reorder Point
safety_stock = z_value * std_demand * np.sqrt(lead_time_weeks)
reorder_point = (avg_demand * lead_time_weeks) + safety_stock

# Display results
print("ðŸ“¦ Inventory Optimization for Store 1, Department 1")
print(f"Average Weekly Demand: {avg_demand:.2f}")
print(f"Demand Standard Deviation: {std_demand:.2f}")
print(f"Lead Time (weeks): {lead_time_weeks}")
print(f"Service Level (Z): {z_value}")
print(f"âœ… Safety Stock: {safety_stock:.2f}")
print(f"âœ… Reorder Point: {reorder_point:.2f}")

import numpy as np

# Filter for Department 1
dept1_df = merged_df[merged_df['Dept'] == 1]

# Parameters
lead_time = 2  # weeks
z_score = 1.65  # for 95% service level

# Calculate inventory metrics per store
results = []
for store_id, group in dept1_df.groupby("Store"):
    weekly_sales = group.sort_values("Date")["Weekly_Sales"]

    avg = weekly_sales.mean()
    std = weekly_sales.std()

    safety_stock = z_score * std * np.sqrt(lead_time)
    reorder_point = (avg * lead_time) + safety_stock

    results.append({
        "Store": store_id,
        "Store Type": group["Type"].iloc[0],
        "Store Size": group["Size"].iloc[0],
        "Average Demand": round(avg, 2),
        "Std Dev": round(std, 2),
        "Safety Stock": round(safety_stock, 2),
        "Reorder Point": round(reorder_point, 2)
    })

comparison_df = pd.DataFrame(results).sort_values(by="Reorder Point", ascending=False)
print(comparison_df.head())

import matplotlib.pyplot as plt
import seaborn as sns

# Set the figure size and style
plt.figure(figsize=(14, 7))
sns.set_style("whitegrid")

# Plot the Reorder Points
sns.barplot(x='Store', y='Reorder Point', data=comparison_df, palette='viridis')

# Add titles and labels
plt.title("ðŸ“¦ Reorder Points for Department 1 - All Stores", fontsize=16)
plt.xlabel("Store", fontsize=12)
plt.ylabel("Reorder Point", fontsize=12)
plt.xticks(rotation=90)

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np

# Step 1: Load your files
train = pd.read_csv("train.csv")
stores = pd.read_csv("stores.csv")
features = pd.read_csv("features.csv")

# Step 2: Convert 'Date' to datetime
train['Date'] = pd.to_datetime(train['Date'])
features['Date'] = pd.to_datetime(features['Date'])

# Step 3: Merge datasets
merged_df = train.merge(stores, on="Store", how="left")
merged_df = merged_df.merge(features, on=["Store", "Date"], how="left")

# Step 4: Filter for Department 2
dept2_df = merged_df[merged_df['Dept'] == 2]

# Step 5: Inventory Optimization Calculations
lead_time = 2 # weeks
z_score = 1.65 # for 95% service level

results = []
for store_id, group in dept2_df.groupby("Store"):
    group = group.sort_values("Date")
    weekly_sales = group["Weekly_Sales"]

    avg_demand = weekly_sales.mean()
    std_dev = weekly_sales.std()

    safety_stock = z_score * std_dev * np.sqrt(lead_time)
    reorder_point = (avg_demand * lead_time) + safety_stock

    results.append({
        "Store": store_id,
        "Store Type": group["Type"].iloc[0],
        "Store Size": group["Size"].iloc[0],
        "Average Demand": round(avg_demand, 2),
        "Std Dev": round(std_dev, 2),
        "Safety Stock": round(safety_stock, 2),
        "Reorder Point": round(reorder_point, 2)
    })

# Step 6: Create a DataFrame
dept2_inventory_df = pd.DataFrame(results)
dept2_inventory_df = dept2_inventory_df.sort_values(by="Reorder Point", ascending=False)

# Display
print(dept2_inventory_df.head())

import matplotlib.pyplot as plt
import pandas as pd

# Replace this data with your actual comparison DataFrame
data = {
    'Store': [10, 4, 27, 14, 20],
    'Reorder Point': [238132.61, 206390.28, 177667.94, 177238.71, 172287.73]
}

df = pd.DataFrame(data)

# Plotting
plt.figure(figsize=(10, 6))
plt.bar(df['Store'].astype(str), df['Reorder Point'], color='skyblue')
plt.xlabel('Store')
plt.ylabel('Reorder Point')
plt.title('Reorder Point Comparison Across Stores (Department 2)')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Sort the DataFrame by Reorder Point in descending order (already done in your screenshot)
sorted_df = comparison_df.sort_values(by='Reorder Point', ascending=False)

# Plotting
plt.figure(figsize=(12, 6))
plt.bar(sorted_df['Store'].astype(str), sorted_df['Reorder Point'], color='skyblue')
plt.xlabel('Store Number')
plt.ylabel('Reorder Point')
plt.title('Reorder Point Comparison Across Stores (Department 1)')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

pd.read_csv("Walmart_Merged.csv")

import numpy as np
from scipy.stats import norm
walmart_data = pd.read_csv("Walmart_Merged.csv")


# If there's a 'Date' column, convert it to datetime
if 'Date' in walmart_data.columns:
    walmart_data['Date'] = pd.to_datetime(walmart_data['Date'])

# Filter data for Store 1
store1_data = walmart_data[walmart_data['Store'] == 1]


# Inventory optimization parameters
z_score = norm.ppf(0.95)  # 95% service level
lead_time = 2  # weeks
#goup by department
Department_groups =store1_data.groupby('Dept')
#initialize the results
results = []
#constants
lead_time = 2
z_score = 1.65

# Calculate metrics by department
metrics = []
for dept, group in store1_data.groupby('Dept'):
    avg_demand = group['Weekly_Sales'].mean()
    std_dev = group['Weekly_Sales'].std()
    safety_stock = z_score * std_dev * np.sqrt(lead_time)
    reorder_point = (avg_demand * lead_time) + safety_stock

    metrics.append({
        'Department': dept,
        'Average Demand': round(avg_demand, 2),
        'Standard Deviation': round(std_dev, 2),
        'Safety Stock': round(safety_stock, 2),
        'Reorder Point': round(reorder_point, 2)
    })

# Convert to DataFrame
store1_inventory_metrics = pd.DataFrame(metrics).sort_values(by='Department')
print(store1_inventory_metrics)

import pandas as pd
import numpy as np
from scipy.stats import norm

# Load the merged data
walmart_data = pd.read_csv("Walmart_Merged.csv")

# Convert 'Date' column to datetime if needed
if 'Date' in walmart_data.columns:
    walmart_data['Date'] = pd.to_datetime(walmart_data['Date'])

# Filter for Store 2
store2_data = walmart_data[walmart_data['Store'] == 2]

# Parameters
lead_time = 2  # in weeks
z_score = 1.65  # for 95% service level

# Calculate metrics for each department
metrics = []

for dept in sorted(store2_data['Dept'].unique()):
    dept_data = store2_data[store2_data['Dept'] == dept]
    avg_demand = dept_data['Weekly_Sales'].mean()
    std_dev = dept_data['Weekly_Sales'].std()

    safety_stock = z_score * std_dev * np.sqrt(lead_time)
    reorder_point = (avg_demand * lead_time) + safety_stock

    metrics.append({
        'Department': dept,
        'Average Demand': round(avg_demand, 2),
        'Standard Deviation': round(std_dev, 2),
        'Safety Stock': round(safety_stock, 2),
        'Reorder Point': round(reorder_point, 2)
    })

# Convert to DataFrame and sort
store2_inventory_metrics = pd.DataFrame(metrics).sort_values(by='Department')
print(store2_inventory_metrics)

import pandas as pd
import numpy as np
from scipy.stats import norm

# Load the merged Walmart data
walmart_merged = pd.read_csv("Walmart_Merged.csv")

# Convert 'Date' column to datetime if not already
if 'Date' in walmart_merged.columns:
    walmart_merged['Date'] = pd.to_datetime(walmart_merged['Date'])

# Filter data for Store 3
store3_data = walmart_merged[walmart_merged['Store'] == 3]

# Set lead time and service level
lead_time = 2  # in weeks
z_score = 1.65  # for 95% service level

# Compute inventory metrics for each department in Store 3
metrics = []
for dept in store3_data['Dept'].unique():
    dept_data = store3_data[store3_data['Dept'] == dept]
    avg_demand = dept_data['Weekly_Sales'].mean()
    std_dev = dept_data['Weekly_Sales'].std()

    safety_stock = z_score * std_dev * np.sqrt(lead_time)
    reorder_point = (avg_demand * lead_time) + safety_stock

    metrics.append({
        'Department': dept,
        'Average Demand': round(avg_demand, 2),
        'Standard Deviation': round(std_dev, 2),
        'Safety Stock': round(safety_stock, 2),
        'Reorder Point': round(reorder_point, 2)
    })

# Convert to DataFrame and sort by Department
store3_inventory_metrics = pd.DataFrame(metrics).sort_values(by='Department')
print(store3_inventory_metrics)

import pandas as pd
import numpy as np

# Load the merged Walmart dataset
df = pd.read_csv("Walmart_Merged.csv")

# Set parameters
lead_time = 2
z_score = 1.65

# Function to calculate inventory metrics for a store
def calculate_inventory_metrics(store_id):
    store_data = df[df['Store'] == store_id]
    metrics = []

    for dept in store_data['Dept'].unique():
        dept_data = store_data[store_data['Dept'] == dept]
        avg_demand = dept_data['Weekly_Sales'].mean()
        std_dev = dept_data['Weekly_Sales'].std()
        safety_stock = z_score * std_dev * np.sqrt(lead_time)
        reorder_point = (avg_demand * lead_time) + safety_stock

        metrics.append({
            'Store': store_id,
            'Department': dept,
            'Average Demand': round(avg_demand, 2),
            'Standard Deviation': round(std_dev, 2),
            'Safety Stock': round(safety_stock, 2),
            'Reorder Point': round(reorder_point, 2)
        })

    return pd.DataFrame(metrics)

# Get inventory metrics for Store 1, 2, and 3
store1_metrics = calculate_inventory_metrics(1)
store2_metrics = calculate_inventory_metrics(2)
store3_metrics = calculate_inventory_metrics(3)

# Combine them into a single DataFrame
combined_metrics = pd.concat([store1_metrics, store2_metrics, store3_metrics])
combined_metrics = combined_metrics.sort_values(by=['Department', 'Store'])

# View results
print(combined_metrics.head(10))  # You can use .to_csv() or visualize as needed

# Save combined inventory metrics as CSV
combined_metrics.to_csv("Inventory_Optimization_Store_1_2_3.csv", index=False)

import pandas as pd

# Load your combined inventory optimization metrics
df = pd.read_csv('Inventory_Optimization_Store_1_2_3.csv')  # Replace with correct path if needed

# Basic summary
print("Summary Statistics:\n")
print(df.describe())

# Insight 1: Departments with highest reorder points
top_reorder = df.sort_values(by="Reorder Point", ascending=False).head(5)
print("\nTop 5 Departments with Highest Reorder Points:")
print(top_reorder[['Store', 'Department', 'Reorder Point']])

# Insight 2: Departments with highest average demand
top_demand = df.sort_values(by="Average Demand", ascending=False).head(5)
print("\nTop 5 Departments with Highest Average Demand:")
print(top_demand[['Store', 'Department', 'Average Demand']])

# Insight 3: Stores with highest overall safety stock
safety_by_store = df.groupby('Store')['Safety Stock'].sum().sort_values(ascending=False)
print("\nTotal Safety Stock by Store:")
print(safety_by_store)

# Insight 4: Variability across stores
std_by_store = df.groupby('Store')['Standard Deviation'].mean()
print("\nAverage Standard Deviation by Store (Demand Variability):")
print(std_by_store)

# Optional: Highlight departments with very low demand but high safety stock
low_demand_high_safety = df[(df['Average Demand'] < 5000) & (df['Safety Stock'] > 10000)]
print("\nDepartments with Low Demand but High Safety Stock:")
print(low_demand_high_safety[['Store', 'Department', 'Average Demand', 'Safety Stock']])

import matplotlib.pyplot as plt

# Pivot the data to get departments as index and stores as columns
pivot_df = combined_metrics.pivot(index='Department', columns='Store', values='Reorder Point')

# Plot
plt.figure(figsize=(14, 7))
pivot_df.plot(kind='bar', figsize=(16, 8))

plt.title("Reorder Point Comparison Across Stores", fontsize=16)
plt.xlabel("Department", fontsize=14)
plt.ylabel("Reorder Point", fontsize=14)
plt.xticks(rotation=45)
plt.legend(title='Store')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

plt.show()

#Total reorder point for each store
total_reorder_by_store = combined_metrics.groupby('Store')['Reorder Point'].sum().sort_values(ascending=False)

print("Total Reorder Point by Store:")
print(total_reorder_by_store)

import matplotlib.pyplot as plt

total_reorder_by_store.plot(kind='bar', color='skyblue')
plt.title('Total Reorder Point by Store')
plt.xlabel('Store')
plt.ylabel('Total Reorder Point')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

!pip install dash==2.11.1
!pip install pyngrok

import pandas as pd
import plotly.express as px
from dash import Dash, dcc, html, Input, Output
from pyngrok import ngrok

pd.read_csv("Walmart_Merged.csv")

!pip install Dash
!pip intall plotly

import pandas as pd
import plotly.express as px
from dash import Dash, dcc, html, Input, Output
import threading
import time
from google.colab import output

# Load your data (replace with your own merged CSV if needed)
data = pd.read_csv('/content/Walmart_Merged.csv')

# Preprocess the data (example: group by store and department)
grouped = data.groupby(['Store', 'Dept']).agg({
    'Weekly_Sales': 'mean',
    'IsHoliday': 'sum'
}).reset_index()
grouped.rename(columns={'Weekly_Sales': 'Average_Sales', 'IsHoliday': 'Holiday_Count'}, inplace=True)

# Start Dash app
app = Dash(__name__)

app.layout = html.Div([
    html.H1("Walmart Store Dashboard"),
    dcc.Dropdown(
        id='store_selector',
        options=[{'label': f'Store {s}', 'value': s} for s in sorted(data['Store'].unique())],
        value=1
    ),
    dcc.Graph(id='sales_bar'),
    dcc.Graph(id='holiday_line')
])

@app.callback(
    Output('sales_bar', 'figure'),
    Output('holiday_line', 'figure'),
    Input('store_selector', 'value')
)
def update_graphs(selected_store):
    filtered = grouped[grouped['Store'] == selected_store]

    fig1 = px.bar(filtered, x='Department', y='Average_Sales',
                  title=f'Average Sales by Department - Store {selected_store}')

    fig2 = px.line(filtered, x='Department', y='Holiday_Count',
                   title=f'Holiday Count by Department - Store {selected_store}')

    return fig1, fig2

from google.colab import output
output.eval_js("window.open('http://localhost:8050', '_blank')")